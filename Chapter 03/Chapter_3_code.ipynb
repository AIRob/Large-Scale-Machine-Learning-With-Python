{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for experimenting yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2 # import urllib.request as urllib2 in Python3\n",
    "import requests, io, os, StringIO\n",
    "import numpy as np\n",
    "import tarfile, zipfile, gzip\n",
    "\n",
    "\n",
    "def unzip_from_UCI(UCI_url, dest=''):\n",
    "    \"\"\"\n",
    "    Downloads and unpacks datasets from UCI in zip format\n",
    "    \"\"\"\n",
    "    response = requests.get(UCI_url)\n",
    "    compressed_file = io.BytesIO(response.content)\n",
    "    z = zipfile.ZipFile(compressed_file)\n",
    "    print ('Extracting in %s' %  os.getcwd()+'\\\\'+dest)\n",
    "    for name in z.namelist():\n",
    "        if '.csv' in name:\n",
    "            print ('\\tunzipping %s' %name)\n",
    "            z.extract(name, path=os.getcwd()+'\\\\'+dest)\n",
    "\n",
    "def gzip_from_UCI(UCI_url, dest=''):\n",
    "    \"\"\"\n",
    "    Downloads and unpacks datasets from UCI in gzip format\n",
    "    \"\"\"\n",
    "    response = urllib2.urlopen(UCI_url)\n",
    "    compressed_file = io.BytesIO(response.read())\n",
    "    decompressed_file = gzip.GzipFile(fileobj=compressed_file)\n",
    "    filename = UCI_url.split('/')[-1][:-3]\n",
    "    with open(os.getcwd()+'\\\\'+filename, 'wb') as outfile:\n",
    "        outfile.write(decompressed_file.read())\n",
    "    print ('File %s decompressed' % filename)\n",
    "            \n",
    "def targzip_from_UCI(UCI_url, dest='.'):\n",
    "    \"\"\"\n",
    "    Downloads and unpacks datasets from UCI in tar.gz format\n",
    "    \"\"\"\n",
    "    response = urllib2.urlopen(UCI_url)\n",
    "    compressed_file = StringIO.StringIO(response.read())\n",
    "    tar = tarfile.open(mode=\"r:gz\", fileobj = compressed_file)\n",
    "    tar.extractall(path=dest)\n",
    "    datasets = tar.getnames()\n",
    "    for dataset in datasets:\n",
    "        size = os.path.getsize(dest+'\\\\'+dataset)\n",
    "        print ('File %s is %i bytes' % (dataset,size))\n",
    "    tar.close()\n",
    "\n",
    "def load_matrix(UCI_url):\n",
    "    \"\"\"\n",
    "    Downloads datasets from UCI in matrix form\n",
    "    \"\"\"\n",
    "    return np.loadtxt(urllib2.urlopen(UCI_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory is: \"C:\\scisoft\\WinPython-64bit-2.7.9.4\\notebooks\\Packt - Large Scale\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print \"Current directory is: \\\"%s\\\"\" % (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "from random import shuffle, seed\n",
    "\n",
    "def ram_shuffle(filename_in, filename_out, header=True, random_seed=0):\n",
    "    with open(filename_in, 'rb') as f:\n",
    "        zlines = [zlib.compress(line, 9) for line in f]\n",
    "        if header:\n",
    "            first_row = zlines.pop(0)\n",
    "    seed(random_seed)\n",
    "    shuffle(zlines)\n",
    "    with open(filename_out, 'wb') as f:\n",
    "        if header:\n",
    "            f.write(zlib.decompress(first_row))\n",
    "        for zline in zlines:\n",
    "            f.write(zlib.decompress(zline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bike Sharing Dataset Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting in C:\\scisoft\\WinPython-64bit-2.7.9.4\\notebooks\\Packt - Large Scale\\bikesharing\n",
      "\tunzipping day.csv\n",
      "\tunzipping hour.csv\n"
     ]
    }
   ],
   "source": [
    "UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "unzip_from_UCI(UCI_url, dest='bikesharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Covertype Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File covtype.data decompressed\n"
     ]
    }
   ],
   "source": [
    "UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "gzip_from_UCI(UCI_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import seed\n",
    "local_path = os.getcwd()\n",
    "source = 'covtype.data'\n",
    "ram_shuffle(filename_in=local_path+'\\\\'+source, \\\n",
    "                   filename_out=local_path+'\\\\shuffled_covtype.data', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Non-linear & faster with Vowpal Wabbit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def apply_log(x): \n",
    "    return np.log(x + 1.0)\n",
    "\n",
    "def apply_exp(x): \n",
    "    return np.exp(x) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | price:.23 sqft:.25 age:.05 2006\n",
      "1 2 'second_house | price:.18 sqft:.15 age:.35 1976\n",
      "0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\n"
     ]
    }
   ],
   "source": [
    "with open('house_dataset','wb') as W:\n",
    "    W.write(\"0 | price:.23 sqft:.25 age:.05 2006\\n\")\n",
    "    W.write(\"1 2 'second_house | price:.18 sqft:.15 age:.35 1976\\n\")\n",
    "    W.write(\"0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\\n\")\n",
    "\n",
    "with open('house_dataset','rb') as R:\n",
    "    for line in R:\n",
    "        print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###A way to call VW from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = house_dataset\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   0.0000   0.0000        5\n",
      "0.666667 1.000000            2            3.0   1.0000   0.0000        5\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 3\n",
      "passes used = 1\n",
      "weighted example sum = 4.000000\n",
      "weighted label sum = 2.000000\n",
      "average loss = 0.750000\n",
      "best constant = 0.500000\n",
      "best constant's loss = 0.250000\n",
      "total feature number = 15\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def execute_vw(parameters):\n",
    "    execution = subprocess.Popen('vw '+parameters, shell=True, stderr=subprocess.PIPE)\n",
    "    line = \"\"\n",
    "    history = \"\"\n",
    "    while True:\n",
    "        out = execution.stderr.read(1)\n",
    "        history += out\n",
    "        if out == '' and execution.poll() != None:\n",
    "            print '------------ COMPLETED ------------\\n'\n",
    "            break\n",
    "        if out != '':\n",
    "            line += out\n",
    "            if '\\n' in line[-2:]:\n",
    "                print line[:-2]\n",
    "                line = ''\n",
    "    return history.split('\\r\\n')\n",
    "\n",
    "\n",
    "params = \"house_dataset\"\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def vw_convert(origin_file, target_file, binary_features, numeric_features, target, transform_target=lambda(x):x,\n",
    "               separator=',', classification=True, multiclass=False, fieldnames= None, header=True, sparse=True):\n",
    "    \"\"\"\n",
    "    Reads a online style stream and returns a generator of normalized feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ‐‐‐‐‐‐‐‐‐‐\n",
    "    original_file = the csv file you are taken the data from \n",
    "    target file = the file to stream from\n",
    "    binary_features = the list of qualitative features to consider\n",
    "    numeric_features = the list of numeric features to consider\n",
    "    target = the label of the response variable\n",
    "    transform_target = a function transforming the response\n",
    "    separator = the field separator character\n",
    "    classification = a Boolean indicating if it is classification\n",
    "    multiclass =  a Boolean indicating if it is multiclass classification\n",
    "    fieldnames = the fields' labels (can be ommitted and read from file)\n",
    "    header = a boolean indicating if the original file has an header\n",
    "    sparse = if a sparse vector is to be returned from the generator\n",
    "    \"\"\"\n",
    "    with open(target_file, 'wb') as W:\n",
    "        with open(origin_file, 'rb') as R:\n",
    "            iterator = csv.DictReader(R, fieldnames, delimiter=separator)\n",
    "            for n, row in enumerate(iterator):\n",
    "                if not header or n>0:\n",
    "                # DATA PROCESSING\n",
    "                    response = transform_target(float(row[target]))\n",
    "                    if classification and not multiclass:\n",
    "                            if response == 0:\n",
    "                                stream_row = '-1 '\n",
    "                            else:\n",
    "                                stream_row = '1 '\n",
    "                    else:\n",
    "                        stream_row = str(response)+' '\n",
    "                    quantitative = list()\n",
    "                    qualitative  = list()\n",
    "                    for k,v in row.iteritems():\n",
    "                        if k in binary_features:\n",
    "                            qualitative.append(str(k)+'_'+str(v)+':1')\n",
    "                        else:\n",
    "                            if k in numeric_features and (float(v)!=0 or not sparse):\n",
    "                                quantitative.append(str(k)+':'+str(v))\n",
    "                    if quantitative:\n",
    "                        stream_row += '|n '+' '.join(quantitative)\n",
    "                    if qualitative:\n",
    "                        stream_row += '|q ' + ' '.join(qualitative)\n",
    "                    W.write(stream_row+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Examples with toys datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from random import seed\n",
    "iris = load_iris()\n",
    "seed(2)\n",
    "re_order = np.random.permutation(len(iris.target))\n",
    "with open('iris_versicolor.vw','wb') as W1:\n",
    "    for k in re_order:\n",
    "        y = iris.target[k]\n",
    "        X = iris.values()[1][k,:]\n",
    "        features = ' |f '+' '.join([a+':'+str(b) for a,b in zip(map(lambda(a): a[:-5].replace(' ','_'), iris.feature_names),X)])\n",
    "        target = '1' if y==1 else '-1'\n",
    "        W1.write(target+features+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "seed(2)\n",
    "re_order = np.random.permutation(len(boston.target))\n",
    "with open('boston.vw','wb') as W1:\n",
    "     for k in re_order:\n",
    "        y = boston.target[k]\n",
    "        X = boston.data[k,:]\n",
    "        features = ' |f '+' '.join([a+':'+str(b) for a,b in zip(map(lambda(a): a[:-5].replace(' ','_'), iris.feature_names),X)])\n",
    "        W1.write(str(y)+features+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Binary Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using l2 regularization = 1e-006\n",
      "predictions = iris_bin.test\n",
      "Lambda = 1e-006\n",
      "Kernel = rbf\n",
      "bandwidth = 0.1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = iris_versicolor.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0  -1.0000   0.0000        5\n",
      "0.960606 0.921212            2            2.0  -1.0000  -0.0788        5\n",
      "1.030685 1.100763            4            4.0  -1.0000  -0.7865        5\n",
      "0.790707 0.550729            8            8.0  -1.0000  -0.3755        5\n",
      "0.647808 0.504909           16           16.0  -1.0000  -1.2473        5\n",
      "0.477695 0.307582           32           32.0   1.0000   0.8621        5\n",
      "0.319804 0.161914           64           64.0  -1.0000  -1.7015        5\n",
      "0.272695 0.225585          128          128.0  -1.0000  -1.3150        5\n",
      "\n",
      "finished run\n",
      "number of examples = 150\n",
      "weighted example sum = 150.000000\n",
      "weighted label sum = -50.000000\n",
      "average loss = 0.248892\n",
      "best constant = -0.333333\n",
      "best constant's loss = 0.888889\n",
      "total feature number = 750\n",
      "Num support = 49\n",
      "Number of kernel evaluations = 8836 Number of cache queries = 18555\n",
      "Total loss = 37.333748\n",
      "Done freeing model\n",
      "Done freeing kernel params\n",
      "Done with finish \n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = '--ksvm --l2 0.000001 --reprocess 2 -b 18 --kernel rbf --bandwidth=0.1 -p iris_bin.test -d iris_versicolor.vw'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "accuracy = 0\n",
    "with open('iris_bin.test', 'rb') as R:\n",
    "    with open('iris_versicolor.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if (n+1) % 10==0:\n",
    "                predicted = float(line.strip())\n",
    "                y = float(example.split('|')[0])\n",
    "                accuracy += np.sign(predicted)==np.sign(y)\n",
    "                holdouts += 1            \n",
    "print 'holdout accuracy: %0.3f' % ((accuracy / holdouts)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = boston.model\n",
      "using dropout for neural network training\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = cache_train.vw\n",
      "Reading datafile = boston.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "2500.000000 2500.000000            1            1.0  50.0000   0.0000        4\n",
      "1570.433136 640.866272            2            2.0  26.4000   1.0847        3\n",
      "945.682968 320.932800            4            4.0  21.0000   3.4834        3\n",
      "738.617393 531.551817            8            8.0  35.4000   6.9177        4\n",
      "559.106543 379.595694           16           16.0  23.1000   6.6911        3\n",
      "362.538769 165.970995           32           32.0  16.7000  12.2397        3\n",
      "301.716126 240.893483           64           64.0  19.7000  12.3789        3\n",
      "236.351873 170.987621          128          128.0  16.1000  15.3972        3\n",
      "180.695258 125.038643          256          256.0  26.5000  24.0065        3\n",
      "99.536619 99.536619          512          512.0  28.7000  18.4439        3 h\n",
      "83.688702 67.840785         1024         1024.0  50.0000  20.8653        4 h\n",
      "72.301786 60.914870         2048         2048.0  10.4000   0.0000        3 h\n",
      "59.041621 45.840391         4096         4096.0  20.6000  21.1746        4 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 456\n",
      "passes used = 10\n",
      "weighted example sum = 4560.000000\n",
      "weighted label sum = 103341.001506\n",
      "average loss = 43.299850 h\n",
      "best constant = 22.662500\n",
      "total feature number = 15220\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = 'boston.vw -f boston.model --loss_function squared -k --cache_file cache_train.vw --passes=20 --nn 5 --dropout'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = boston.test\n",
      "using dropout for neural network testing\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache_test.vw\n",
      "Reading datafile = boston.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "922.607483 922.607483            1            1.0  50.0000  19.6255        4\n",
      "464.302045 5.996608            2            2.0  26.4000  23.9512        3\n",
      "253.949617 43.597188            4            4.0  21.0000  21.2530        3\n",
      "175.713928 97.478239            8            8.0  35.4000  25.5958        4\n",
      "130.466937 85.219947           16           16.0  15.2000  15.8726        3\n",
      "79.291346 28.115755           32           32.0  15.6000  19.7057        4\n",
      "85.270478 91.249610           64           64.0  22.8000  20.4866        3\n",
      "83.265921 81.261364          128          128.0  20.8000  18.1267        3\n",
      "70.838572 58.411224          256          256.0  27.5000  16.6386        3\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 506\n",
      "passes used = 1\n",
      "weighted example sum = 506.000000\n",
      "weighted label sum = 11401.600174\n",
      "average loss = 65.960779\n",
      "best constant = 22.532808\n",
      "total feature number = 1687\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = '-t boston.vw -i boston.model -k --cache_file cache_test.vw -p boston.test'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout RMSE: 7.010\n"
     ]
    }
   ],
   "source": [
    "val_rmse = 0\n",
    "with open('boston.test', 'rb') as R:\n",
    "    with open('boston.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if (n+1) % 10==0:\n",
    "                predicted = float(line.strip())\n",
    "                y = float(example.split('|')[0])\n",
    "                val_rmse += (predicted - y)**2\n",
    "                holdouts += 1            \n",
    "print 'holdout RMSE: %0.3f' % ((val_rmse / holdouts)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bike sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "local_path = os.getcwd()\n",
    "b_vars = ['holiday','hr','mnth', 'season','weathersit','weekday','workingday','yr']\n",
    "n_vars = ['hum', 'temp', 'atemp', 'windspeed']\n",
    "source = '\\\\bikesharing\\\\hour.csv'\n",
    "origin = target_file=local_path+'\\\\'+source\n",
    "target = target_file=local_path+'\\\\'+'bike.vw'\n",
    "vw_convert(origin, target, binary_features=b_vars, numeric_features=n_vars, target = 'cnt', transform_target=apply_log,\n",
    "               separator=',', classification=False, multiclass=False, fieldnames= None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = regression.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = cache_train.vw\n",
      "Reading datafile = bike.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "8.027098 8.027098            1            1.0   2.8332   0.0000       12\n",
      "7.243733 6.460369            2            2.0   3.7136   1.1718       12\n",
      "4.184013 1.124293            4            4.0   2.6391   2.4762       12\n",
      "2.709537 1.235061            8            8.0   1.3863   1.5636       12\n",
      "2.265795 1.822052           16           16.0   4.7095   3.7598       13\n",
      "1.325281 0.384768           32           32.0   2.1972   1.5774       13\n",
      "1.350559 1.375836           64           64.0   5.0626   3.8186       13\n",
      "1.395717 1.440876          128          128.0   4.2195   4.0547       13\n",
      "1.165618 0.935518          256          256.0   2.0794   3.3485       13\n",
      "0.952714 0.739810          512          512.0   4.0775   3.6438       13\n",
      "0.757944 0.563175         1024         1024.0   5.4116   4.0760       13\n",
      "0.583856 0.409769         2048         2048.0   1.0986   1.0007       13\n",
      "0.453590 0.323324         4096         4096.0   5.4027   5.5651       13\n",
      "0.393729 0.333867         8192         8192.0   3.8286   4.1227       12\n",
      "0.561750 0.561750        16384        16384.0   4.3944   4.0809       13 h\n",
      "0.509105 0.456460        32768        32768.0   4.4659   4.4656       13 h\n",
      "0.468332 0.427559        65536        65536.0   4.5951   4.4378       13 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 15999\n",
      "passes used = 6\n",
      "weighted example sum = 95994.000000\n",
      "weighted label sum = 439183.191893\n",
      "average loss = 0.427485 h\n",
      "best constant = 4.575111\n",
      "total feature number = 1235898\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = 'bike.vw -f regression.model -k --cache_file cache_train.vw --passes=1000 --hash strings --holdout_after 16000'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = pred.test\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache_test.vw\n",
      "Reading datafile = bike.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.127379 0.127379            1            1.0   2.8332   3.1901       12\n",
      "0.751745 1.376112            2            2.0   3.7136   2.5405       12\n",
      "1.210345 1.668944            4            4.0   2.6391   1.5334       12\n",
      "2.774795 4.339245            8            8.0   1.3863   4.3803       12\n",
      "2.276018 1.777242           16           16.0   4.7095   4.8526       13\n",
      "2.179675 2.083333           32           32.0   2.1972   4.6568       13\n",
      "1.411963 0.644251           64           64.0   5.0626   5.1554       13\n",
      "0.836451 0.260938          128          128.0   4.2195   4.6608       13\n",
      "0.677186 0.517921          256          256.0   2.0794   2.8816       13\n",
      "0.600932 0.524678          512          512.0   4.0775   4.0583       13\n",
      "0.512835 0.424738         1024         1024.0   5.4116   4.8593       13\n",
      "0.498590 0.484345         2048         2048.0   1.0986   1.0587       13\n",
      "0.422767 0.346943         4096         4096.0   5.4027   5.7840       13\n",
      "0.407376 0.391985         8192         8192.0   3.8286   3.9312       12\n",
      "0.374806 0.342236        16384        16384.0   5.7900   5.4536       12\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 17379\n",
      "passes used = 1\n",
      "weighted example sum = 17379.000000\n",
      "weighted label sum = 79504.382239\n",
      "average loss = 0.380562\n",
      "best constant = 4.574739\n",
      "total feature number = 223723\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = '-t bike.vw -i regression.model -k --cache_file cache_test.vw -p pred.test'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout RMSE: 135.306\n",
      "holdout RMSLE: 0.845\n"
     ]
    }
   ],
   "source": [
    "val_rmse = 0\n",
    "val_rmsle = 0\n",
    "with open('pred.test', 'rb') as R:\n",
    "    with open('bike.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if n > 16000:\n",
    "                predicted = float(line.strip())\n",
    "                y_log = float(example.split('|')[0])\n",
    "                y = apply_exp(y_log)\n",
    "                val_rmse += (apply_exp(predicted) - y)**2\n",
    "                val_rmsle += (predicted - y_log)**2\n",
    "                holdouts += 1\n",
    "            \n",
    "print 'holdout RMSE: %0.3f' % ((val_rmse / holdouts)**0.5)\n",
    "print 'holdout RMSLE: %0.3f' % ((val_rmsle / holdouts)**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "local_path = os.getcwd()\n",
    "n_vars = ['var_'+'0'*int(j<10)+str(j) for j in range(54)]\n",
    "source = 'shuffled_covtype.data'\n",
    "origin = target_file=local_path+'\\\\'+source\n",
    "target = target_file=local_path+'\\\\'+'covtype.vw'\n",
    "vw_convert(origin, target, binary_features=list(), fieldnames= n_vars+['covertype'], numeric_features=n_vars,\n",
    "    target = 'covertype', separator=',', classification=True, multiclass=True, header=False, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cubic features for triples: nnn \n",
      "final_regressor = multiclass.model\n",
      "Num weight bits = 18\n",
      "learning rate = 1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = cache_train.vw\n",
      "Reading datafile = covtype.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      377\n",
      "0.000000 0.000000            2            2.0        1        1      377\n",
      "0.250000 0.500000            4            4.0        2        1      377\n",
      "0.375000 0.500000            8            8.0        1        2      377\n",
      "0.437500 0.500000           16           16.0        2        1      231\n",
      "0.531250 0.625000           32           32.0        1        2      377\n",
      "0.546875 0.562500           64           64.0        2        1      377\n",
      "0.500000 0.453125          128          128.0        1        1      377\n",
      "0.519531 0.539063          256          256.0        2        2      377\n",
      "0.484375 0.449219          512          512.0        2        2      377\n",
      "0.446289 0.408203         1024         1024.0        3        6      377\n",
      "0.416504 0.386719         2048         2048.0        2        2      377\n",
      "0.402100 0.387695         4096         4096.0        1        1      377\n",
      "0.372559 0.343018         8192         8192.0        1        1      298\n",
      "0.348694 0.324829        16384        16384.0        1        1      377\n",
      "0.319092 0.289490        32768        32768.0        2        2      377\n",
      "0.297256 0.275421        65536        65536.0        2        2      377\n",
      "0.278419 0.259583       131072       131072.0        2        2      377\n",
      "0.263660 0.248901       262144       262144.0        2        2      377\n",
      "0.253858 0.253858       524288       524288.0        1        1      377 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 522911\n",
      "passes used = 2\n",
      "weighted example sum = 1045822.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.235538 h\n",
      "total feature number = 384838154\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = 'covtype.vw --ect 7 -f multiclass.model -k --cache_file cache_train.vw --passes=2 -l 1.0 --cubic nnn'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cubic features for triples: nnn \n",
      "only testing\n",
      "predictions = covertype.test\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache_test.vw\n",
      "Reading datafile = covtype.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      377\n",
      "0.000000 0.000000            2            2.0        1        1      377\n",
      "0.000000 0.000000            4            4.0        2        2      377\n",
      "0.000000 0.000000            8            8.0        1        1      377\n",
      "0.187500 0.375000           16           16.0        1        2      377\n",
      "0.156250 0.125000           32           32.0        3        3      377\n",
      "0.156250 0.156250           64           64.0        2        1      377\n",
      "0.218750 0.281250          128          128.0        2        2      377\n",
      "0.222656 0.226563          256          256.0        2        2      377\n",
      "0.240234 0.257813          512          512.0        2        2      377\n",
      "0.234375 0.228516         1024         1024.0        2        2      377\n",
      "0.242676 0.250977         2048         2048.0        2        2      377\n",
      "0.242920 0.243164         4096         4096.0        1        1      377\n",
      "0.236328 0.229736         8192         8192.0        1        1      377\n",
      "0.231079 0.225830        16384        16384.0        1        1      298\n",
      "0.229858 0.228638        32768        32768.0        1        1      377\n",
      "0.232224 0.234589        65536        65536.0        1        1      377\n",
      "0.231529 0.230835       131072       131072.0        2        2      377\n",
      "0.231815 0.232101       262144       262144.0        2        2      377\n",
      "0.231606 0.231396       524288       524288.0        1        1      377\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 581012\n",
      "passes used = 1\n",
      "weighted example sum = 581012.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.231111\n",
      "total feature number = 213797603\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = '-t covtype.vw -i multiclass.model -k --cache_file cache_test.vw -p covertype.test'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holdout accuracy: 0.769\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "with open('covertype.test', 'rb') as R:\n",
    "    with open('covtype.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if (n+1) % 10==0:\n",
    "                predicted = float(line.strip())\n",
    "                y = float(example.split('|')[0])\n",
    "                accuracy += predicted ==y\n",
    "                holdouts += 1\n",
    "print 'holdout accuracy: %0.3f' % (accuracy / holdouts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
